#!/usr/bin/perl

use strict;
use lib "$ENV{GUS_HOME}/lib/perl";
use Getopt::Long;
use GUS::Supported::GusConfig;
use DBI;

$| = 1;

my ($gusConfigFile, $dropApiDB, $dropGUS, $create, $db, $dbVendor, $allowFailures, $verbose);

&GetOptions("gusConfigFile=s" => \$gusConfigFile,
           "dropApiDB!" => \$dropApiDB,
           "dropGUS!" => \$dropGUS,
           "allowFailures!" => \$allowFailures,
           "create!" => \$create,
           "verbose!" => \$verbose,
           "db=s" => \$db);

my $x = $dropApiDB + $dropGUS + $create;
if (!$db || ($x != 1)) {
  die "

Install the ApiDB and ApiDBTuning schemas and the ApidDB patches to GUS, or uninstall them (or uninstall the GUS schemas)

usage: installApidbSchema --db database [--create | --dropApiDB | --dropGUS] [--gusConfigFile gus_config_file] [--allowFailures] [--verbose]

Caution: only use --allowFailures if you know what you are doing

create - creates the ApiDB and ApiDBTuning schemas and patches GUS.

dropApiDB - drops the ApiDB and ApiDBTuning schemas, and ApiDB patches to GUS.

dropGUS - drops the Core, DoTS, PROT, RAD, SRes, STUDY, TESS (and all VER) schemas

Log is printed to STDOUT

Verbose prints commands run to STDERR

";
}

my $d = 'GUS schema';
$d = 'ApiDB and ApiDBTuning schemas' if $dropApiDB;

if ($dropApiDB || $dropGUS) {
    print "\nYou are about to UNINSTALL the $d on:\n  $db\nType the name of the instance to confirm: ";
    my $confirm = <STDIN>;
    chomp $confirm;
    die "You did not correctly confirm the db instance\n" unless $db eq $confirm;
}

my $gusconfig = GUS::Supported::GusConfig->new($gusConfigFile);

my $login = $gusconfig->getDatabaseLogin();
my $password = $gusconfig->getDatabasePassword();
my $dbVendor = $gusconfig->getDatabaseVendor();
my $dbiDsn = $gusconfig->getDbiDsn();


# PLEASE READ BEFORE ADDING TO @create
#
# The list is ordered according to these rules:
# first:  changes to GUS
# second: create ApidbTuning schema
# third:  create Apidb schema
# fourth: create tables within Apidb, in dependency order as needed
#
# This ordering helps ensure that when/if we drop, the dropping proceeds
# in the correct order, avoiding bogus failures from dropping what was
# not yet created.
#
# ALSO: if you are adding a create script for the Apidb schema
#       it is good practice to write a parallel drop script
#       so that if needed, the create can be manually reversed

my @create = qw(
  removeNullConstraintsFromSimilarityTables.sql
  createGusTuning.sql
  createApidbTuningSchema.sql
  createApidbUserDatasetsSchema.sql
  createTestTuningSchema.sql
  createApidbSchema.sql
  createChebiSchema.sql
  createHmdbSchema.sql
  createEdaSchema.sql
  createHMDBTables.sql
  createAnalysisMethodInvocation.sql
  createDatasource.sql
  createSnpTables.sql
  createGeneInteractionTables.sql
  createGeneFeatureProduct.sql
  createGeneFeatureName.sql
  createOrthologGroup.sql
  createPhylogeneticProfile.sql
  createPhylogeneticTree.sql
  createRelatedNaFeature.sql
  createReportCache.sql
  createSequenceAttributeTables.sql
  createOrganism.sql
  createStoredProcedures.sql
  createPivotProcedure.sql
  createSyntenyTables.sql
  createSpliceSiteFeature.sql
  createPubChemSubstance.sql
  createPubChemCompound.sql
  createDbRefCompound.sql
  createOldAnnotation.sql
  createOldCodingSequence.sql
  createTuningManager.sql
  createWorkflow.sql
  createIntronJunction.sql
  createNAFeatureHaploblock.sql
  createBlatProteinAlignment.sql
  createGFF3Table.sql
  createEcNumberGenus.sql
  createSpliceSiteGenes.sql
  createPolyAGenes.sql
  createMassSpecSummary.sql
  createCompoundMassSpec.sql
  createPathwayTables.sql
  createTranscriptProduct.sql
  createIsolateGPS.sql
  createPhenotype.sql
  createNAFeaturePhenotype.sql
  createPhenotypeScore.sql
  createPhenotypeMutants.sql
  createPhenotypeGrowthRate.sql
  createHaplotypeResult.sql
  createOntologyTermResult.sql
  createSubjectResult.sql
  createSeqEdit.sql
  createWHOStandards.sql
  createLOPITResults.sql
  createWGCNAResults.sql
  createWGCNAModuleEigengeneResults.sql
  createNAFeatureMetaCycle.sql
  createCNVTables.sql
  createRflpTables.sql
  createNAFeatureImage.sql
  createSequenceTaxon.sql
  createFeatureLocation.sql
  createCrisprPhenotype.sql
  createGoSubset.sql
  createVdiControlTables.sql
  createUserDatasetCoreTables.sql
  createUserDatasetTypeTables.sql
  createDatabaseTableMapping.sql
  createEntityGraphTables.sql
  createEdaUdModifications.sql
  createExternalResourceUrl.sql
  createAGPPiece.sql
  createAlphaFoldTable.sql
  createLegacyDatasetNames.sql
  createBusco.sql
  createCellxgene.sql
  grantChebiTables.sql
);


# drop GUS first, then ApidbTuning, then Apidb (reverse of order they were created in).
# for GUS, we only drop the tuning indexes, as the other GUS changes don't
# need to be reversed.  The trick here is to order these to avoid failures
my @delete = qw(
  restoreNullConstraintsFromSimilarityTables.sql
  dropGusTuning.sql
  dropDbLinks.sql
  dropTestTuningSchema.sql
  dropApidbTuningSchema.sql
  dropApidbUserDatasetsSchema.sql
  dropVdiControlTables.sql
  dropApidbSchema.sql
  dropChebiSchema.sql
  dropHMDBSchema.sql
  dropExternalResourceUrl.sql
);

my @gusSchemas = qw(
  CORE
  DOTS
  MODEL
  PLATFORM
  RESULTS
  SRES
  STUDY
  COREVER
  DOTSVER
  MODELVER
  PLATFORMVER
  RESULTSVER
  SRESVER
  STUDYVER
);

my @apiDbSchemas = qw(
  APIDB
  APIDBTUNING
  APIDBUSERDATASETS
  CHEBI
  HMDB
  EDA
  EDA_UD
  TESTTUNING
);

if ($create) {
  for my $sql (@create) {
     print STDOUT "\n==============================================================\n";
     print STDOUT "$sql\n";     
     print STDOUT "==============================================================\n";
     
     my @sqlplusParamValues;
     if($sql eq 'createEntityGraphTables.sql') {
       $sqlplusParamValues[0] = "EDA";
       $sqlplusParamValues[1] = "SRES";
       &runSql($login, $password, $db, $sql, $dbVendor, @sqlplusParamValues);



       $sqlplusParamValues[0] = "ApidbUserDatasets";
       $sqlplusParamValues[1] = "ApidbUserDatasets";
       &runSql($login, $password, $db, $sql, $dbVendor, @sqlplusParamValues);
     }

     else {
       &runSql($login, $password, $db, $sql, $dbVendor, @sqlplusParamValues);
     }
  }
} elsif ($dropApiDB) {
  # &dropSchemaSetTables($login, $password, $db, "'APIDB','APIDBTUNING','APIDBUSERDATASETS','CHEBI','HMDB', 'EDA', 'EDA_UD'");
    if (lc $dbVendor eq 'oracle') {
      my $schemaSetStr = join ', ', map "'$_'", @apiDbSchemas;
      &dropSchemaSetTables($login, $password, $db, $schemaSetStr);
    } elsif (lc $dbVendor eq 'postgres') {
      &dropSchemaSetPostgres($dbiDsn, $login, $password, $db, @apiDbSchemas);
    } else {
      die "Unsupported dbVendor:$dbVendor.";
    }
} elsif ($dropGUS) {
  # &dropSchemaSet($login, $password, $db,"'CORE','DOTS','MODEL','PLATFORM','RESULTS','SRES','STUDY','COREVER','DOTSVER','MODELVER','PLATFORMVER','RESULTSVER','SRESVER','STUDYVER'");
  if (lc $dbVendor eq 'oracle') {
    my $schemaSetStr = join ', ', map "'$_'", @gusSchemas;
    &dropSchemaSetOracle($login, $password, $db, $schemaSetStr);
  } elsif (lc $dbVendor eq 'postgres') {
    &dropSchemaSetPostgres($dbiDsn, $login, $password, $db, @gusSchemas);
  } else {
    die "Unsupported dbVendor:$dbVendor.";
  }
}

print STDERR "\nDone.\n";

sub runSql {
  my ($login, $password, $db, $file, $dbVendor, @params) = @_;
  
  if (lc $dbVendor eq 'oracle') { 
    &runSqlOracle($login, $password, $db, $dbVendor, $file, @params);
  } elsif (lc $dbVendor eq 'postgres') {
    &runSqlPostgres($login, $password, $db, $dbVendor, $file, @params);
  } else { 
    die "Unsupported dbVendor:$dbVendor."; 
  }
}

sub runSqlOracle {
  my ($login, $password, $db, $dbVendor, $file, @params) = @_;

  my $fullFile = "$ENV{GUS_HOME}/lib/sql/apidbschema/$dbVendor/$file";

  -e $fullFile || die "File .sql file '$fullFile' does not exist\n";

  my $tmpFile = "/tmp/$file.$$";  # append the process id
  unlink($tmpFile);  # in case of a old one
  my $cmd;
  if (!$allowFailures) {
      $cmd = "echo 'whenever sqlerror exit sql.sqlcode;' > $tmpFile";
      runCmd($cmd, $tmpFile);
  }
  $cmd = "echo 'set echo on;' >> $tmpFile";
  runCmd($cmd, $tmpFile);

  $cmd = "cat $fullFile >> $tmpFile";
  runCmd($cmd, $tmpFile);

  # my $sqlplusParamValuesString = join(" ", @sqlplusParamValues);
  $cmd = "sqlplus $login\@$db/$password \@$tmpFile " . join(' ', @params);
  print STDOUT "\n==============================================================\n";
  print STDOUT "Running $tmpFile\n";
  print STDOUT "==============================================================\n";

  runCmd($cmd, $tmpFile);
  unlink($tmpFile);
}

sub runSqlPostgres {
  my ($login, $password, $db, $dbVendor, $file, @params) = @_;
  
  my $psql_params = "";
  $dbiDsn =~ /(:|;)host=((\w|\.)+);?/ ;
  my $hostname = $2;
  my $connectionString = "postgresql://$login:$password\@$hostname/$db";
  
  my $fullFile = "$ENV{GUS_HOME}/lib/sql/apidbschema/$dbVendor/$file";
  -e $fullFile || die "File .sql file '$fullFile' does not exist\n";

  my $cmd;
  if (!$allowFailures) { $psql_params = "-v ON_ERROR_STOP=1"; }

  if (scalar @params > 0) {
    my @paramsFull;
    for(my $i = 0; $i < scalar @params; $i++) {
      my $n = $i +1;
      my $p = "VAR${n}";
      push @paramsFull, "-v ${p}=$params[$i]";
    }

    $psql_params = "$psql_params " . join(" ", @paramsFull);
  }

  $cmd = "psql --echo-all -f $fullFile $psql_params $connectionString";
  print STDOUT "\n==============================================================\n";
  print STDOUT "Running $fullFile\n";
  print STDOUT "==============================================================\n";

  runCmd($cmd, $fullFile);
}

sub runCmd {
    my ($cmd, $tmpFile) = @_;
    print STDERR "\nrunning command: $cmd\n" if $verbose;
    system($cmd);
    my $status = $? >> 8;
    if ($status) {
      unlink($tmpFile) if $cmd =~ /^sqlplus/;
      die "Failed with status '$status running cmd: \n$cmd'\n";
    }
}

sub dropSchemaSetTables {
  my ($login, $password, $db, $schemaSet) = @_;

  my $dsn = "dbi:Oracle:" . $db;
  my $dbh = DBI->connect(
                $dsn,
                $login,
                $password,
                { PrintError => 1, RaiseError => 0}
                ) or die "Can't connect to the database: $DBI::errstr\n";

  print STDERR "\nfixing to drop objectss in schema set \"$schemaSet\"\n" if $verbose;

  # drop everything
  my $stmt = $dbh->prepare(<<SQL);
    select 'drop ' || object_type || ' ' || owner || '.' || object_name
           || decode(object_type, 'TABLE', ' CASCADE CONSTRAINTS', '')
    from all_objects
    where owner in ($schemaSet)
      and object_type not in ('INDEX', 'TRIGGER', 'TYPE BODY')
SQL

  my $objectsToDrop = 1;

  while ($objectsToDrop) {
    $stmt->execute() or print STDERR "\n" . $dbh->errstr . "\n";

    if (my ($dropStmtSql) = $stmt->fetchrow_array()) {
      print STDERR "running statement: $dropStmtSql\n" if $verbose;
      $dbh->do($dropStmtSql) or die "Can't fetch a drop statement: $DBI::errstr\n";
    } else {
      $objectsToDrop = 0;
    }
  }

}

sub dropSchemaSetOracle {
  my ($login, $password, $db, $schemaSet) = @_;

  my $dsn = "dbi:Oracle:" . $db;
  my $dbh = DBI->connect(
                $dsn,
                $login,
                $password,
                { PrintError => 1, RaiseError => 0}
                ) or die "Can't connect to the database: $DBI::errstr\n";

  print STDERR "\nfixing to drop schemas in set \"$schemaSet\"\n" if $verbose;

  my $stmt = $dbh->prepare(<<SQL);
    select 'drop user ' || username || ' cascade'
    from all_users
    where username in ($schemaSet)
SQL

  $stmt->execute() or print STDERR "\n" . $dbh->errstr . "\n";

  while (my ($dropStmtSql) = $stmt->fetchrow_array()) {
    print STDERR "\nrunning statement: $dropStmtSql\n" if $verbose;
    $dbh->do($dropStmtSql) or die "Can't fetch a drop statement: $DBI::errstr\n";
  }
}

sub dropSchemaSetPostgres {
  my ($dbiDsn, $login, $password, $db, @schemaSet) = @_;

  my $dbh = DBI->connect(
                $dbiDsn,
                $login,
                $password,
                { PrintError => 1, RaiseError => 0}
                ) or die "Can't connect to the database: $DBI::errstr\n";

  print STDERR "\nfixing to drop schemas in set \"@schemaSet\"\n" if $verbose;

  for my $schema (@schemaSet) {
    my $stmt = $dbh->prepare(<<SQL);
    DROP SCHEMA $schema CASCADE;
SQL

    $stmt->execute() or print STDERR "\n" . $dbh->errstr . "\n";

    while (my ($dropStmtSql) = $stmt->fetchrow_array()) {
      print STDERR "\nrunning statement: $dropStmtSql\n" if $verbose;
      $dbh->do($dropStmtSql) or die "Can't fetch a drop statement: $DBI::errstr\n";
    }
  }
}
